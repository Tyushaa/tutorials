{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5053c0-9a04-4d4b-af6a-6964d84c1f65",
   "metadata": {},
   "source": [
    "# LangChain API Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use LangChain's core functionalities.\n",
    "LangChain is a powerful framework designed to facilitate building language model-powered applications.\n",
    "We'll explore its components, including prompt creation, chains, retrieval, and agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37718519",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "We'll start by setting up the environment and initializing our language model (`ChatOpenAI`).\n",
    "Here, we're using OpenAI's `gpt-4o-mini` model with a temperature of 0 for deterministic outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc0a856-9e3b-4001-92dd-68f739434a1a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#!sudo /venv/bin/pip install langchain --quiet\n",
    "#!sudo /venv/bin/pip install -U langchain-community --quiet\n",
    "#!sudo /venv/bin/pip install -U langchain-openai --quiet\n",
    "#!sudo /venv/bin/pip install --quiet chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3cf220-31e0-4782-b66d-9df5d10685bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import helpers.hdbg as hdbg\n",
    "import langchain\n",
    "import langchain.agents as lngchagents\n",
    "import langchain.document_loaders.csv_loader as csvloader\n",
    "import langchain.prompts as lngchprmt\n",
    "import langchain.schema.messages as lnchscme\n",
    "import langchain.schema.runnable as lngchschrun\n",
    "import langchain_community.vectorstores as vectorstores\n",
    "import langchain_core.output_parsers as lngchoutpar\n",
    "import langchain_openai as lngchopai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efcb5daf-ebf6-4574-8037-eeb7222b580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mWARNING: Running in Jupyter\n",
      "INFO  > cmd='/venv/lib/python3.12/site-packages/ipykernel_launcher.py -f /home/.local/share/jupyter/runtime/kernel-8c2761ac-1946-448c-8889-66f927fb8c54.json'\n"
     ]
    }
   ],
   "source": [
    "hdbg.init_logger(verbosity=logging.INFO)\n",
    "\n",
    "_LOG = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09cfe05-5c9d-4d48-ab7e-c9fafd34a6c5",
   "metadata": {},
   "source": [
    "## Add your OpenAPI key and initiate GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e73ca7-6473-4abe-b903-8036d2099256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add OpenAPI to environment variable.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openapi-key\"\n",
    "# Initiate OpenAI model.\n",
    "chat_model = lngchopai.ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea58a6",
   "metadata": {},
   "source": [
    "## Message Handling with `HumanMessage` and `SystemMessage`\n",
    "\n",
    "LangChain uses `HumanMessage` and `SystemMessage` objects to structure conversations.\n",
    "- **`SystemMessage`**: Defines the behavior of the assistant.\n",
    "- **`HumanMessage`**: Represents user input.\n",
    "\n",
    "Let's see how this works in practice with some example messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59dc1486-f47f-462f-a507-219137b443ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Medicaid managed care is a system of delivering Medicaid services through private health insurance plans. In this model, state Medicaid programs contract with private managed care organizations (MCOs) to provide a range of healthcare services to Medicaid beneficiaries. Here are some key features of Medicaid managed care:\\n\\n1. **Care Coordination**: MCOs are responsible for coordinating care for their members, which can help improve health outcomes and reduce unnecessary hospitalizations.\\n\\n2. **Cost Control**: Managed care aims to control costs by negotiating rates with providers and managing the utilization of services. This can help states manage their Medicaid budgets more effectively.\\n\\n3. **Access to Services**: Beneficiaries typically have access to a network of providers, which may include primary care physicians, specialists, hospitals, and other healthcare services. Members may need to choose a primary care provider (PCP) and get referrals for specialist care.\\n\\n4. **Preventive Services**: Many managed care plans emphasize preventive care and wellness services to help members maintain their health and avoid more costly interventions later.\\n\\n5. **Quality Improvement**: MCOs often implement quality improvement initiatives to enhance the care provided to their members, focusing on metrics such as patient satisfaction, health outcomes, and adherence to clinical guidelines.\\n\\n6. **State Flexibility**: States have the flexibility to design their Medicaid managed care programs, including the types of services covered, payment structures, and the populations served.\\n\\n7. **Enrollment**: Medicaid beneficiaries may be automatically enrolled in a managed care plan or have the option to choose from several plans available in their area.\\n\\nOverall, Medicaid managed care aims to provide comprehensive, coordinated, and cost-effective healthcare services to low-income individuals and families who qualify for Medicaid.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 346, 'prompt_tokens': 24, 'total_tokens': 370, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_27322b4e16', 'id': 'chatcmpl-BFBXXDMVttTKo4dcAIst4ZhN4ql6i', 'finish_reason': 'stop', 'logprobs': None}, id='run-e8fcc3b6-3735-471b-a598-27db8e69954c-0', usage_metadata={'input_tokens': 24, 'output_tokens': 346, 'total_tokens': 370, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define system behavior and user input.\n",
    "messages = [\n",
    "    lnchscme.SystemMessage(\n",
    "        content=\"You're an assistant knowledgeable about healthcare.\"\n",
    "    ),\n",
    "    lnchscme.HumanMessage(content=\"What is Medicaid managed care?\"),\n",
    "]\n",
    "# Generate a response.\n",
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc351315",
   "metadata": {},
   "source": [
    "## Restricting Assistant's Scope\n",
    "\n",
    "You can further control the assistant's responses by tailoring the `SystemMessage`.\n",
    "For instance, we'll restrict it to only answer healthcare-related questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccc34d4c-72df-4be5-97a9-f201d334524c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm here to assist with healthcare-related questions. If you have any health concerns or questions about medical topics, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 31, 'total_tokens': 58, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_27322b4e16', 'id': 'chatcmpl-BFBXd4JH6r1a3ypqnuZNj4nL4Iari', 'finish_reason': 'stop', 'logprobs': None}, id='run-129daa36-0ed7-4448-affc-87502c23c89d-0', usage_metadata={'input_tokens': 31, 'output_tokens': 27, 'total_tokens': 58, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You're an assistant knowledgeable about healthcare. Only answer healthcare-related questions.\"\n",
    "    ),\n",
    "    HumanMessage(content=\"How do I change a tire?\"),\n",
    "]\n",
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d825081",
   "metadata": {},
   "source": [
    "## Creating Custom Prompts with `ChatPromptTemplate`\n",
    "\n",
    "Custom prompts are essential for tasks like summarization, question answering, or content generation.\n",
    "We'll use `ChatPromptTemplate` to define structured prompts and format them dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1916da5-713c-4c19-827f-d700319cb3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
      "Use the following context to answer questions. Be as detailed as possible, but don't make up\n",
      "any information that's not from the context. If you don't know an answer, say you don't know.\n",
      "\n",
      "I had a great stay!\n",
      "\n",
      "Did anyone have a positive experience?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a custom template for hospital reviews.\n",
    "review_template_str = \"\"\"\n",
    "Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
    "Use the following context to answer questions. Be as detailed as possible, but don't make up\n",
    "any information that's not from the context. If you don't know an answer, say you don't know.\n",
    "\n",
    "{context}\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "review_template = lngchprmt.ChatPromptTemplate.from_template(review_template_str)\n",
    "\n",
    "# Provide context and a question.\n",
    "context = \"I had a great stay!\"\n",
    "question = \"Did anyone have a positive experience?\"\n",
    "\n",
    "# Format the template.\n",
    "print(review_template.format(context=context, question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9685209",
   "metadata": {},
   "source": [
    "## Advanced Prompt Structuring\n",
    "\n",
    "Sometimes, you may need to structure prompts with multiple components, like system instructions and user input.\n",
    "`SystemMessagePromptTemplate` and `HumanMessagePromptTemplate` allow fine-grained control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3217aad3-3b15-4d2c-93f3-5194493ef5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"\\nYour job is to use patient reviews to answer questions about their experience at a hospital.\\nUse the following context to answer questions. Be as detailed as possible, but don't make up\\nany information that's not from the context. If you don't know an answer, say you don't know.\\n\\nI had a great stay!\\n\\nDid anyone have a positive experience?\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Did anyone have a positive experience?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Define system and human message templates.\n",
    "review_system_prompt = lngchprmt.SystemMessagePromptTemplate(\n",
    "    prompt=lngchprmt.PromptTemplate(\n",
    "        input_variables=[\"context\"], template=review_template_str\n",
    "    )\n",
    ")\n",
    "\n",
    "review_human_prompt = lngchprmt.HumanMessagePromptTemplate(\n",
    "    prompt=lngchprmt.PromptTemplate(\n",
    "        input_variables=[\"question\"], template=\"{question}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combine into a chat prompt template.\n",
    "review_prompt_template = lngchprmt.ChatPromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    messages=[review_system_prompt, review_human_prompt],\n",
    ")\n",
    "\n",
    "# Format messages.\n",
    "formatted_messages = review_prompt_template.format_messages(\n",
    "    context=context, question=question\n",
    ")\n",
    "print(formatted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db841519-c997-4d8a-969b-88fc094d03aa",
   "metadata": {},
   "source": [
    "## Chains\n",
    "\n",
    "Chains are a fundamental abstraction in LangChain, allowing you to combine prompts and models into workflows.\n",
    "We'll create a simple chain to answer questions based on a given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2db8f73-e443-400b-996b-8b7865fa7de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, one patient mentioned that they had a great stay at the hospital, indicating a positive experience.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a chain using the template and chat model.\n",
    "output_parser = lngchoutpar.StrOutputParser()\n",
    "review_chain = review_prompt_template | chat_model | output_parser\n",
    "\n",
    "# Test the chain.\n",
    "review_chain.invoke({\"context\": context, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50dcf87c-ea70-4e43-9f02-683c9f3d9925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anidhi\t  ck_marketing\tdanya\tjsmeriga  pavolr    sameepp  tomaz\n",
      "chutianm  dan\t\tgrisha\tninat\t  samarthk  sonaalk  vedanshuj\n"
     ]
    }
   ],
   "source": [
    "!ls /shared_data/dev/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2999afc",
   "metadata": {},
   "source": [
    "## Retrieval with FAISS\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is used to efficiently retrieve relevant documents based on similarity scores.\n",
    "We'll demonstrate how to load a dataset, create embeddings, and retrieve documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4eca8f64-fd83-41f5-ae96-d2c25f41da5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/Users/saggese/src/github/langchain_neo4j_rag_app/data/reviews.csv': No such file or directory\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error loading build_LLM_RAG_chatbot_with_langchain/reviews.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/lib/python3.12/site-packages/langchain_community/document_loaders/csv_loader.py:134\u001b[39m, in \u001b[36mCSVLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__read_file(csvfile)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'build_LLM_RAG_chatbot_with_langchain/reviews.csv'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load reviews dataset\u001b[39;00m\n\u001b[32m      6\u001b[39m loader = CSVLoader(file_path=REVIEWS_CSV_PATH, source_column=\u001b[33m\"\u001b[39m\u001b[33mreview\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m reviews = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create a vector store\u001b[39;00m\n\u001b[32m     10\u001b[39m reviews_vector_db = Chroma.from_documents(reviews, OpenAIEmbeddings(), persist_directory=REVIEWS_CHROMA_PATH)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/lib/python3.12/site-packages/langchain_core/document_loaders/base.py:32\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/lib/python3.12/site-packages/langchain_community/document_loaders/csv_loader.py:151\u001b[39m, in \u001b[36mCSVLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    149\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Error loading build_LLM_RAG_chatbot_with_langchain/reviews.csv"
     ]
    }
   ],
   "source": [
    "#!cp /Users/saggese/src/github/langchain_neo4j_rag_app/data/reviews.csv build_LLM_RAG_chatbot_with_langchain/.\n",
    "REVIEWS_CSV_PATH = \"build_LLM_RAG_chatbot_with_langchain/reviews.csv\"\n",
    "REVIEWS_CHROMA_PATH = \"chroma_data\"\n",
    "\n",
    "# Load reviews dataset.\n",
    "loader = csvloader.CSVLoader(file_path=REVIEWS_CSV_PATH, source_column=\"review\")\n",
    "reviews = loader.load()\n",
    "\n",
    "# Create a vector store.\n",
    "reviews_vector_db = vectorstores.Chroma.from_documents(\n",
    "    reviews, lngchopai.OpenAIEmbeddings(), persist_directory=REVIEWS_CHROMA_PATH\n",
    ")\n",
    "\n",
    "# Retrieve relevant documents.\n",
    "question = \"Has anyone complained about communication with the hospital staff?\"\n",
    "relevant_docs = reviews_vector_db.similarity_search(question, k=3)\n",
    "\n",
    "print(relevant_docs[0].page_content)\n",
    "print(relevant_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5db6e-b21b-40d8-8416-a4b367d7590f",
   "metadata": {},
   "source": [
    "## Building a Retrieval-Enhanced QA Chain\n",
    "\n",
    "We'll combine retrieval and prompts to build a more dynamic question-answering system.\n",
    "This chain retrieves relevant documents and uses them as context for the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d1b143-17c9-4b58-befa-a2f69257e374",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create a retriever.\n",
    "reviews_retriever = reviews_vector_db.as_retriever(k=10)\n",
    "\n",
    "# Build the QA chain.\n",
    "review_chain = (\n",
    "    {\"context\": reviews_retriever, \"question\": lngchschrun.RunnablePassthrough()}\n",
    "    | review_prompt_template\n",
    "    | chat_model\n",
    "    | lngchoutpar.StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test the QA chain.\n",
    "question = \"Has anyone complained about communication with the hospital staff?\"\n",
    "review_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9988de7a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Agents\n",
    "\n",
    "Agents are more flexible than chains, as they can decide the sequence of actions to take.\n",
    "We'll demonstrate an agent that can answer questions using tools for reviews and wait times.\n",
    "\n",
    "- The chain is hardwired\n",
    "- An agent is an LLM that decides the sequence of actions to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98d4ac-8727-45f6-992d-4513d9714d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_wait_time(hospital: str) -> int | str:\n",
    "    \"\"\"\n",
    "    Dummy function to generate fake wait times.\n",
    "\n",
    "    :param\n",
    "    \"\"\"\n",
    "    if hospital not in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        return f\"Hospital {hospital} does not exist\"\n",
    "    # Simulate API call delay.\n",
    "    time.sleep(1)\n",
    "    return random.randint(0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b812a5c8-03db-4cbd-b7e3-c553933bb27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool is an interface that an agent uses to interact with a function.\n",
    "# Each description explains the Agent when to call each tool.\n",
    "tools = [\n",
    "    lngchagents.Tool(\n",
    "        name=\"Reviews\",\n",
    "        func=review_chain.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions\n",
    "        about patient reviews or experiences at the hospital.\n",
    "        Not useful for answering questions about specific visit\n",
    "        details such as payer, billing, treatment, diagnosis,\n",
    "        chief complaint, hospital, or physician information.\n",
    "        Pass the entire question as input to the tool. For instance,\n",
    "        if the question is \"What do patients think about the triage system?\",\n",
    "        the input should be \"What do patients think about the triage system?\"\n",
    "        \"\"\",\n",
    "    ),\n",
    "    lngchagents.Tool(\n",
    "        name=\"Waits\",\n",
    "        func=get_current_wait_time,\n",
    "        description=\"\"\"Use when asked about current wait times\n",
    "        at a specific hospital. This tool can only get the current\n",
    "        wait time at a hospital and does not have any information about\n",
    "        aggregate or historical wait times. This tool returns wait times in\n",
    "        minutes. Do not pass the word \"hospital\" as input,\n",
    "        only the hospital name itself. For instance, if the question is\n",
    "        \"What is the wait time at hospital A?\", the input should be \"A\".\n",
    "        \"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "hospital_agent_prompt = langchain.hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "agent_chat_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "hospital_agent = lngchagents.create_openai_functions_agent(\n",
    "    llm=agent_chat_model,\n",
    "    prompt=hospital_agent_prompt,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "# Agent run-time.\n",
    "hospital_agent_executor = lngchagents.AgentExecutor(\n",
    "    agent=hospital_agent,\n",
    "    tools=tools,\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f6d8e-f7f9-4237-97e9-b8dcc25cbe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_agent_executor.invoke(\n",
    "    {\"input\": \"What is the current wait time at hospital C?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53094d4-5868-4085-9ca5-95379954338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_agent_executor.invoke(\n",
    "    {\"input\": \"What have patients said about their comfort at the hospital?\"}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
