{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b412cd",
   "metadata": {},
   "source": [
    "# Building a Documentation Chatbot with LangChain\n",
    "\n",
    "This script demonstrates how to build an intelligent chatbot that queries documentation using LangChain.\n",
    "The chatbot can:\n",
    "- Parse and preprocess Markdown files.\n",
    "- Embed document content for efficient similarity-based retrieval.\n",
    "- Answer detailed, context-aware queries from users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532dfea4-93b4-41e2-b6c1-5d7f57a4140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo /venv/bin/pip install langchain --quiet\n",
    "#!sudo /venv/bin/pip install -U langchain-community --quiet\n",
    "#!sudo /venv/bin/pip install -U langchain-openai --quiet\n",
    "#!sudo /venv/bin/pip install -U langchain-core --quiet\n",
    "#!sudo /venv/bin/pip install -U langchainhub --quiet\n",
    "#!sudo /venv/bin/pip install -U unstructured python-magic pandoc markdown faiss-cpu --quiet\n",
    "#!sudo /venv/bin/pip install --quiet chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cafda052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "from typing import Dict, List\n",
    "\n",
    "import helpers.hdbg as hdbg\n",
    "import langchain\n",
    "import langchain.chains\n",
    "import langchain.docstore.document as lngchdocstordoc\n",
    "import langchain.embeddings\n",
    "import langchain.hub\n",
    "import langchain.text_splitter\n",
    "import langchain_openai\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "014b1dd4-f9ec-4263-8f53-657e4a7f64ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mWARNING: Running in Jupyter\n",
      "INFO  > cmd='/venv/lib/python3.12/site-packages/ipykernel_launcher.py -f /home/.local/share/jupyter/runtime/kernel-6fc94654-baa5-45f8-82dd-06e0bdd4fb28.json'\n"
     ]
    }
   ],
   "source": [
    "hdbg.init_logger(verbosity=logging.INFO)\n",
    "\n",
    "_LOG = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a42c7e-203e-406d-a315-e5853fd2f000",
   "metadata": {},
   "source": [
    "## Define Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a40064-5415-49c9-b2d5-354a839b9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"open_ai_api_key\": \"\",\n",
    "    # Define language model arguments.\n",
    "    \"language_model\": {\n",
    "        # Define your model here.\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"temperature\": 0,\n",
    "    },\n",
    "    # Define input directory path containing documents.\n",
    "    \"source_directory\": \"../../helpers_root/docs\",\n",
    "    \"parse_data_into_chunks\": {\n",
    "        \"chunk_size\": 500,\n",
    "        \"chunk_overlap\": 50,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7208ea51-4419-4137-89a2-5f2756c14f1d",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "We'll begin by importing the required libraries and configuring the environment. The chatbot will use:\n",
    "- OpenAI's GPT-4o-mini as the core language model.\n",
    "- FAISS for fast document retrieval.\n",
    "- LangChain utilities for document parsing, text splitting, and chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac548ed5-8093-43b0-a498-e17ebb61b0e0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set the OpenAI API key.\n",
    "os.environ[\"OPENAI_API_KEY\"] = config[\"open_ai_api_key\"]\n",
    "# Initialize the chat model.\n",
    "chat_model = langchain_openai.ChatOpenAI(**config[\"language_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c3a1e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea92dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_markdown_files(dir_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively list all markdown files in a directory.\n",
    "\n",
    "    :param dir_path: path to directory containing markdown files\n",
    "    :return: list of absolute paths to markdown files\n",
    "    \"\"\"\n",
    "    md_files = []\n",
    "    for root, _, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):\n",
    "                md_files.append(str(pathlib.Path(root) / file))\n",
    "    _LOG.info(\"Found %d markdown files in %s\", len(md_files), dir_path)\n",
    "    return md_files\n",
    "\n",
    "\n",
    "def initialize_known_files(dir_path: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Create initial known_files state with existing markdown files.\n",
    "\n",
    "    :param dir_path: path to directory containing markdown files\n",
    "    :return: dictionary of known files and their modification times\n",
    "    \"\"\"\n",
    "    known_files = {}\n",
    "    for file_path in list_markdown_files(dir_path):\n",
    "        path = pathlib.Path(file_path)\n",
    "        known_files[str(path)] = path.stat().st_mtime\n",
    "    return known_files\n",
    "\n",
    "\n",
    "def parse_markdown_files(file_paths: List[str]) -> List[lngchdocstordoc.Document]:\n",
    "    \"\"\"\n",
    "    Parse markdown files into LangChain Documents with metadata.\n",
    "\n",
    "    :param file_paths: list of paths to markdown files\n",
    "    :return: list of Document objects with content and metadata\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            loader = UnstructuredMarkdownLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"source\"] = file_path\n",
    "                doc.metadata[\"last_modified\"] = os.path.getmtime(file_path)\n",
    "                doc.metadata[\"checksum\"] = hashlib.md5(\n",
    "                    doc.page_content.encode()\n",
    "                ).hexdigest()\n",
    "            documents.extend(docs)\n",
    "        except Exception as e:\n",
    "            _LOG.error(\"Error loading %s: %s\", file_path, str(e))\n",
    "    _LOG.info(\"Successfully parsed %d/%d files\", len(documents), len(file_paths))\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    documents: List[lngchdocstordoc.Document],\n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 50,\n",
    ") -> List[lngchdocstordoc.Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks using text splitter.\n",
    "\n",
    "    :param documents: list of Documents to split\n",
    "    :param chunk_size: size of each chunk in characters\n",
    "    :param chunk_overlap: overlap between chunks in characters\n",
    "    :return: list of chunked Document objects\n",
    "    \"\"\"\n",
    "    text_splitter = langchain.text_splitter.RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    _LOG.info(\"Split %d documents into %d chunks\", len(documents), len(chunks))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_vector_store(\n",
    "    documents: List[lngchdocstordoc.Document],\n",
    "    embeddings: langchain.embeddings.OpenAIEmbeddings,\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Create FAISS vector store from documents.\n",
    "\n",
    "    :param documents: list of Document objects\n",
    "    :param embeddings: embeddings model to use\n",
    "    :return: FAISS vector store\n",
    "    \"\"\"\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    _LOG.info(\"Created vector store with %d entries\", len(documents))\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def build_retriever(vector_store: FAISS, search_kwargs: Dict = {\"k\": 4}):\n",
    "    \"\"\"\n",
    "    Build retriever from vector store.\n",
    "\n",
    "    :param vector_store: FAISS vector store\n",
    "    :param search_kwargs: keyword arguments for retriever\n",
    "    :return: retriever\n",
    "    \"\"\"\n",
    "    retriever = vector_store.as_retriever(search_kwargs=search_kwargs)\n",
    "    _LOG.info(\"Built retriever with config: %s\", search_kwargs)\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def watch_folder_for_changes(\n",
    "    dir_path: str, known_files: Dict[str, float]\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Monitor directory for file changes.\n",
    "\n",
    "    :param dir_path: path to directory to monitor\n",
    "    :param known_files: dictionary of known files and their modification\n",
    "        times\n",
    "    :return: dictionary of changed files\n",
    "    \"\"\"\n",
    "    # Get current files in directory.\n",
    "    current_files = {\n",
    "        str(p): p.stat().st_mtime for p in pathlib.Path(dir_path).rglob(\"*.md\")\n",
    "    }\n",
    "    # Detect changes.\n",
    "    changes = {\n",
    "        \"new\": [],\n",
    "        \"modified\": [],\n",
    "        \"deleted\": list(known_files.keys() - current_files.keys()),\n",
    "    }\n",
    "    for path, mtime in current_files.items():\n",
    "        if path not in known_files:\n",
    "            changes[\"new\"].append(path)\n",
    "        elif mtime > known_files[path]:\n",
    "            changes[\"modified\"].append(path)\n",
    "    # Update known files.\n",
    "    known_files.update(current_files)\n",
    "    return changes\n",
    "\n",
    "\n",
    "def update_vector_store(\n",
    "    vector_store: FAISS,\n",
    "    new_documents: List[lngchdocstordoc.Document],\n",
    "    embeddings: langchain.embeddings.OpenAIEmbeddings,\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Update existing vector store with new documents.\n",
    "\n",
    "    :param vector_store: FAISS vector store\n",
    "    :param new_documents: list of new Document objects\n",
    "    :param embeddings: embeddings model to use\n",
    "    :return: updated FAISS vector store\n",
    "    \"\"\"\n",
    "    if new_documents:\n",
    "        new_vector_store = FAISS.from_documents(new_documents, embeddings)\n",
    "        vector_store.merge_from(new_vector_store)\n",
    "        _LOG.info(\"Added %d new documents to vector store\", len(new_documents))\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542cfc8",
   "metadata": {},
   "source": [
    "## Parse and Preprocess Documentation\n",
    "\n",
    "Markdown files serve as the primary data source for this chatbot.\n",
    "We'll parse the files into LangChain `Document` objects and split them into manageable chunks to ensure efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5f91f41-afe7-49fa-9859-397009613558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  Found 101 markdown files in ../../helpers_root/docs\n",
      "INFO  Successfully parsed 101/101 files\n",
      "INFO  Split 101 documents into 1818 chunks\n"
     ]
    }
   ],
   "source": [
    "# Initialize with documents\n",
    "md_files = list_markdown_files(config[\"source_directory\"])\n",
    "raw_documents = parse_markdown_files(md_files)\n",
    "chunked_documents = split_documents(\n",
    "    raw_documents,\n",
    "    chunk_size=config[\"parse_data_into_chunks\"][\"chunk_size\"],\n",
    "    chunk_overlap=config[\"parse_data_into_chunks\"][\"chunk_overlap\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31074294",
   "metadata": {},
   "source": [
    "## Create a FAISS Vector Store\n",
    "\n",
    "To enable fast document retrieval, we'll embed the document chunks using OpenAI's embeddings and store them in a FAISS vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63a3a326-140a-4543-8f01-155e0fa36192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_567/1465729325.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = langchain.embeddings.OpenAIEmbeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO  HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO  Loading faiss with AVX512 support.\n",
      "INFO  Successfully loaded faiss with AVX512 support.\n",
      "INFO  Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes.\n",
      "INFO  Created vector store with 1818 entries\n",
      "INFO  FAISS vector store created with 1818 documents.\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI embeddings.\n",
    "embeddings = langchain.embeddings.OpenAIEmbeddings()\n",
    "# Create a FAISS vector store.\n",
    "vector_store = create_vector_store(chunked_documents, embeddings)\n",
    "_LOG.info(\"FAISS vector store created with %d documents.\", len(chunked_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23fd80",
   "metadata": {},
   "source": [
    "## Build a QA Chain\n",
    "\n",
    "The `RetrievalQA` chain combines document retrieval with OpenAI's GPT-3.5 for question answering.\n",
    "It retrieves the most relevant document chunks and uses them as context to generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bab54142-1414-4d82-a6e1-1a51db624dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  Built retriever with config: {'k': 4}\n",
      "INFO  RetrievalQA chain initialized.\n"
     ]
    }
   ],
   "source": [
    "# Build the retriever from the vector store\n",
    "retriever = build_retriever(vector_store)\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = langchain.chains.RetrievalQA.from_chain_type(\n",
    "    llm=chat_model, retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "_LOG.info(\"RetrievalQA chain initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e262aba",
   "metadata": {},
   "source": [
    "## Step 5: Query the Chatbot\n",
    "\n",
    "Let's interact with the chatbot! We'll ask it questions based on the documentation.\n",
    "The chatbot will retrieve relevant chunks and generate context-aware responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "427c7461-6d5c-4125-a2f4-2a57e5b62b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_567/1840385247.py:5: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO  HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Answer:\n",
      "The guidelines for setting up a new project include:\n",
      "\n",
      "1. **Planning**: Before starting any work, sit down and plan. Describe your high-level plan in writing, ideally in a Google doc for easier collaboration and review.\n",
      "\n",
      "2. **Define Objectives**:\n",
      "   - Clearly outline what the code should do.\n",
      "   - Identify the functionalities you want to implement.\n",
      "   - Specify the functionalities that are out-of-scope.\n",
      "\n",
      "3. **Prioritization**: Determine what is more important and what is less important, categorizing tasks into P0, P1, P2.\n",
      "\n",
      "4. **Project Creation**:\n",
      "   - Click on the green New Project button.\n",
      "   - In the pop-up, choose Project templates > From your organization and select the appropriate template (e.g., [TEMPLATE] Kaizen Project).\n",
      "   - Change the default name to a relevant and short name that represents the project.\n",
      "\n",
      "5. **Timeline**: Aim to complete the project within 2-3 months to avoid long-running projects.\n",
      "\n",
      "6. **Quality Requirements**: Establish how good you want the software to be, keeping in mind that good software today is better than perfect software tomorrow.\n",
      "\n",
      "7. **Project Details**: \n",
      "   - Click on the Project details button and provide a short description of the project, including the customer, overarching goal, team leader, and key resources (e.g., Slack/Asana channel).\n",
      "   - Add team members to the project and manage access by inviting collaborators.\n",
      "\n",
      "8. **Big Picture Awareness**: Remember the big picture to avoid projects getting out of hand and missing deadlines.\n",
      "\n",
      "By following these guidelines, you can set up a structured and effective project.\n",
      "\n",
      "Source Documents:\n",
      "- Source: ../../helpers_root/docs/coding/all.code_design.how_to_guide.md\n",
      "  Excerpt: Measure seven times, cut once (Russian proverb)\n",
      "\n",
      "Before doing any work, sit down and plan\n",
      "\n",
      "Describe somewhere in writing your high-level plan. Put it in a Google doc to make it easier to collaborate a\n",
      "- Source: ../../helpers_root/docs/work_organization/ck.github_projects_process.reference.md\n",
      "  Excerpt: Click on the green New Project button\n",
      "\n",
      "A new window with a pop-up should appear, in the pop-up choose Project templates > From your organization and choose [TEMPLATE] Kaizen Project\n",
      "\n",
      "Change the defaul\n",
      "- Source: ../../helpers_root/docs/coding/all.code_like_pragmatic_programmer.how_to_guide.md\n",
      "  Excerpt: It's the so-called \"startup fatigue\"\n",
      "\n",
      "It's easier to ask forgiveness, than it is to get permission\n",
      "\n",
      "PP_Tip 6: Remember the big picture\n",
      "\n",
      "Projects slowly and inexorably get totally out of hand\n",
      "\n",
      "Missing \n",
      "- Source: ../../helpers_root/docs/work_organization/ck.github_projects_process.reference.md\n",
      "  Excerpt: Click on the Project details button in the upper right corner next to the three dots ...\n",
      "\n",
      "Provide a short description of the project, e.g.,who is the customer\n",
      "\n",
      "What is the overarching goal of the proj\n"
     ]
    }
   ],
   "source": [
    "# Define a user query.\n",
    "query = \"What are the guidelines for setting up a new project?\"\n",
    "\n",
    "# Query the chatbot.\n",
    "response = qa_chain({\"query\": query})\n",
    "\n",
    "# Display the answer and source documents.\n",
    "print(f\"Answer:\\n{response['result']}\\n\")\n",
    "print(\"Source Documents:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(f\"- Source: {doc.metadata['source']}\")\n",
    "    print(f\"  Excerpt: {doc.page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d2ca4",
   "metadata": {},
   "source": [
    "## Step 6: Dynamic Updates\n",
    "\n",
    "What if the documentation changes? We'll handle this by monitoring the folder for new or modified files.\n",
    "The vector store will be updated dynamically to ensure the chatbot stays up-to-date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb2c959f-0def-4a38-b329-34bf7f3206bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  Found 101 markdown files in ../../helpers_root/docs\n"
     ]
    }
   ],
   "source": [
    "# Monitor the folder for changes and update the vector store.\n",
    "known_files = initialize_known_files(config[\"source_directory\"])\n",
    "\n",
    "# Detect changes using our custom watcher.\n",
    "changes = watch_folder_for_changes(\n",
    "    dir_path=config[\"source_directory\"], known_files=known_files\n",
    ")\n",
    "\n",
    "if changes[\"new\"] or changes[\"modified\"]:\n",
    "    # Process changed files.\n",
    "    changed_files = changes[\"new\"] + changes[\"modified\"]\n",
    "    # Parse markdown files using our custom parser.\n",
    "    raw_new_docs = parse_markdown_files(changed_files)\n",
    "    if raw_new_docs:\n",
    "        # Split into chunks using configured parameters.\n",
    "        chunked_new_docs = split_documents(\n",
    "            documents=raw_new_docs,\n",
    "            chunk_size=config[\"parse_data_into_chunks\"][\"chunk_size\"],\n",
    "            chunk_overlap=config[\"parse_data_into_chunks\"][\"chunk_overlap\"],\n",
    "        )\n",
    "        # Update vector store with new chunks.\n",
    "        update_vector_store(\n",
    "            vector_store=vector_store,\n",
    "            new_documents=chunked_new_docs,\n",
    "            embeddings=embeddings,\n",
    "        )\n",
    "        _LOG.info(\n",
    "            \"Updated vector store with %d new chunks from %d files\",\n",
    "            len(chunked_new_docs),\n",
    "            len(changed_files),\n",
    "        )\n",
    "        _LOG.debug(\"New/modified files: %s\", changed_files)\n",
    "    else:\n",
    "        _LOG.warning(\n",
    "            \"No valid documents found in %d changed files\", len(changed_files)\n",
    "        )\n",
    "# Handle deleted files if needed.\n",
    "if changes[\"deleted\"]:\n",
    "    _LOG.warning(\n",
    "        \"Deletion handling not implemented. Found %d deleted files: %s\",\n",
    "        len(changes[\"deleted\"]),\n",
    "        changes[\"deleted\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260331ce",
   "metadata": {},
   "source": [
    "## Step 7: Enhancements - Personalization\n",
    "\n",
    "We can extend the chatbot to include personalized responses:\n",
    "- Filter documents by metadata (e.g., tags, categories).\n",
    "- Customize responses based on user preferences.\n",
    "\n",
    "For example, users can ask for specific sections of the documentation or request summaries tailored to their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e2de353-2f56-41e4-bf35-4d6c036bd539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO  HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Answer:\n",
      "I don't know.\n",
      "\n",
      "Source Documents:\n",
      "- Source: ../../helpers_root/docs/onboarding/all.onboarding_checklist.reference.md\n",
      "  Excerpt: Onboarding Checklist\n",
      "\n",
      "Onboarding process for a new team member\n",
      "\n",
      "Meta\n",
      "\n",
      "Make on-boarding automatic\n",
      "\n",
      "Be patient\n",
      "\n",
      "Ask for confirmation\n",
      "\n",
      "Make on-boarding similar to our work routine\n",
      "\n",
      "Improve on-boarding pr\n",
      "- Source: ../../helpers_root/docs/onboarding/all.onboarding_checklist.reference.md\n",
      "  Excerpt: Ask for confirmation of all the actions, e.g.,\n",
      "\n",
      "\"Does this and that work?\"\n",
      "\n",
      "\"Did you receive the email?\"\n",
      "\n",
      "\"Can you log in?\"\n",
      "\n",
      "Make the new team member follow the instructions so that they can get famil\n",
      "- Source: ../../helpers_root/docs/onboarding/ck.hiring_process.how_to_guide.md\n",
      "  Excerpt: Follow the instructions in all.onboarding_checklist.reference.md\n",
      "\n",
      "HiringMeister: once the full onboarding is complete, organize more complex tasks to test their development and problem-solving skills\n",
      "\n",
      "- Source: ../../helpers_root/docs/onboarding/all.onboarding_checklist.reference.md\n",
      "  Excerpt: We use it only to track the time automatically and as HR\n",
      "\n",
      "[ ] Team member:\n",
      "\n",
      "Confirm access to Hubstaff\n",
      "\n",
      "Read Instructions for Hubstaff\n",
      "\n",
      "IT setup\n",
      "\n",
      "[ ] Team leader: File an issue with this checklist\n",
      "\n",
      "Th\n"
     ]
    }
   ],
   "source": [
    "# Example query with personalized intent.\n",
    "personalized_query = \"Show me onboarding guidelines for new employees.\"\n",
    "\n",
    "# Query the chatbot.\n",
    "personalized_response = qa_chain({\"query\": personalized_query})\n",
    "\n",
    "# Display the personalized response.\n",
    "print(f\"Answer:\\n{personalized_response['result']}\\n\")\n",
    "print(\"Source Documents:\")\n",
    "for doc in personalized_response[\"source_documents\"]:\n",
    "    print(f\"- Source: {doc.metadata['source']}\")\n",
    "    print(f\"  Excerpt: {doc.page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e7b90",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this script, we:\n",
    "1. Parsed and processed Markdown documentation.\n",
    "2. Embedded document chunks into a FAISS vector store for efficient retrieval.\n",
    "3. Built a RetrievalQA chain for context-aware question answering.\n",
    "4. Enabled dynamic updates to handle changing documentation.\n",
    "5. Enhanced the chatbot with personalized query handling.\n",
    "\n",
    "This showcases how LangChain can be used to build intelligent, flexible chatbots tailored for specific tasks."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
