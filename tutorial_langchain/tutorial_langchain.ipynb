{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b412cd",
   "metadata": {},
   "source": [
    "# Tutorial For Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f91f41-afe7-49fa-9859-397009613558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23fd80",
   "metadata": {},
   "source": [
    "### Define the GPT Model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab54142-1414-4d82-a6e1-1a51db624dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "import glob\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "427c7461-6d5c-4125-a2f4-2a57e5b62b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def parse_markdown_files(file_paths) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Parse all the markdown files into Documents.\n",
    "\n",
    "    :param file_paths: list of md file_paths\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        # Create a Document object for each file\n",
    "        documents.append(Document(page_content=content, metadata={\"source\": file_path}))\n",
    "    return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260331ce",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter \n",
    "Utility function in LangChain  for splitting large chunks of text into smaller more manageable pieces while ensuring minimal overlap or fragmentation of meaningful content.\n",
    "\n",
    "### Key Features\n",
    "1. **Recursive Splitting**: \n",
    "   - It splits the text hierarchically using multiple delimiters. The splitting process starts with the most significant delimiter (e.g., paragraph breaks) and progressively moves to less significant ones (e.g., sentence breaks, word breaks).\n",
    "   - This ensures that the text is split cleanly and logically, retaining semantic coherence as much as possible.\n",
    "\n",
    "2. **Customizable Delimiters**:\n",
    "   - You can specify a list of delimiters (e.g., `\\n\\n`, `. `, `, `) for the splitting process.\n",
    "   - The splitter uses these in order, falling back to smaller units if a larger split would result in chunks exceeding the maximum size.\n",
    "\n",
    "3. **Chunk Size and Overlap**:\n",
    "   - `chunk_size`: The maximum length of each text chunk, typically measured in characters.\n",
    "   - `chunk_overlap`: The number of characters to overlap between consecutive chunks. This helps in preserving context when chunks are processed individually.\n",
    "\n",
    "4. **Text Preprocessing**:\n",
    "   - Trims unnecessary whitespace around chunks.\n",
    "   - Ensures no chunk exceeds the defined `chunk_size`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e2de353-2f56-41e4-bf35-4d6c036bd539",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../docs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# List Markdown files\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m markdown_files \u001b[38;5;241m=\u001b[39m \u001b[43mlist_markdown_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Parse Markdown files into LangChain documents\u001b[39;00m\n\u001b[1;32m     11\u001b[0m documents \u001b[38;5;241m=\u001b[39m parse_markdown_files(markdown_files)\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36mlist_markdown_files\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlist_markdown_files\u001b[39m(directory):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/*.md\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "def list_markdown_files(directory):\n",
    "    return list(glob.glob(f\"{directory}/*.md\"))\n",
    "\n",
    "# Directory containing Markdown files\n",
    "directory = \"../docs\"\n",
    "\n",
    "# List Markdown files\n",
    "markdown_files = list_markdown_files(directory)\n",
    "\n",
    "# Parse Markdown files into LangChain documents\n",
    "documents = parse_markdown_files(markdown_files)\n",
    "\n",
    "# Split long documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Print sample chunked documents\n",
    "for doc in split_documents[:5]:\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e7b90",
   "metadata": {},
   "source": [
    "### VECTOR STORES\n",
    "\n",
    "#### FAISS (Facebook AI Similarity Search) \n",
    "It is a library designed for efficient similarity search and clustering of dense vectors. In LangChain, FAISS is commonly used as a vector store to store and retrieve embeddings, which are vector representations of text or other data.\n",
    "\n",
    "#### Key Features of FAISS Vector Stores:\n",
    "1. Efficient Storage and Search: FAISS stores dense vector embeddings and allows fast retrieval using similarity metrics like cosine similarity or inner product.\n",
    "2. Indexing Options: Supports different types of indexes (e.g., Flat, IVF, HNSW) to balance between accuracy and speed depending on the dataset size and search requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be86c16-f178-42cf-b0c0-911c84c0d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Embed and store split_documents\n",
    "vector_store = FAISS.from_documents(split_documents, embeddings)\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4a0dc-967f-4225-ba19-54f444498bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create the QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8920b1c6-1f29-4092-a4ce-672fdc339033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User's question\n",
    "query = \"What are the guidelines on creating new project\"\n",
    "\n",
    "# Get the answer and source documents\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the answer\n",
    "print(\"Answer:\")\n",
    "print(result['result'])\n",
    "\n",
    "# Print the source file references\n",
    "print(\"\\nSource Documents:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(f\"File: {doc.metadata['source']}\")\n",
    "    print(f\"Excerpt: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63c5db-d452-4392-bd4f-c68da5b3b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve vectors by document name\n",
    "def get_vectors_by_document_name(vector_store, document_name):\n",
    "    # Query using the metadata field `source`\n",
    "    results = vector_store.similarity_search(\n",
    "        query=\"\",  # Pass an empty query or a dummy vector if supported\n",
    "        k=None,    # Retrieve all matching documents\n",
    "        filter={\"source\": document_name}  # Filter by the document name\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "document_name = \"all.how_write_tutorials.how_to_guide.md\"\n",
    "results = get_vectors_by_document_name(vector_store, document_name)\n",
    "\n",
    "# Print results\n",
    "for doc in results:\n",
    "    print(f\"File: {doc.metadata['source']}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e68a944",
   "metadata": {},
   "source": [
    "### Demo to create a documentation QA bot but the docs can be updated or deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057dc61-2130-4395-ad63-9af8690698f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = None\n",
    "folder = \"../docs\"\n",
    "filename_to_md5sum = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7458e-e47e-4513-a5e0-caa3b5dae7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers.hsystem as hsystem\n",
    "# Function to parse and structure Markdown files. \n",
    "def parse_markdown_files(file_paths):\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        md5sum, _ = hsystem.system_to_string(f\"md5sum {file_path}\")[1].split()\n",
    "        filename_to_md5sum[file_path] = md5sum\n",
    "        # Create a Document object for each file\n",
    "        documents.append(Document(page_content=content, metadata={\"source\": file_path}))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae283e-6aff-49bc-ad21-fe8db669eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "def create_vector_store_from_markdown_files(folder):\n",
    "    # List Markdown files\n",
    "    markdown_files = list_markdown_files(directory)\n",
    "    # Parse Markdown files into LangChain documents\n",
    "    documents = parse_markdown_files(markdown_files)\n",
    "    # Split long documents into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    split_documents = text_splitter.split_documents(documents)\n",
    "    # Create embeddings for all documents.\n",
    "    vector_store = Chroma.from_documents(split_documents, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0661b8-4e40-4e07-9cbd-b54cad041170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_changes_in_documents_folder(folder):\n",
    "    # List Markdown files\n",
    "    markdown_files = list_markdown_files(folder)\n",
    "    changes = {}\n",
    "    changes[\"modified\"] = []\n",
    "    for file_path in markdown_files:\n",
    "        md5sum, _ = hsystem.system_to_string(f\"md5sum {file_path}\")[1].split()\n",
    "        if file_path not in filename_to_md5sum or filename_to_md5sum[file_path] == md5sum:\n",
    "            print(f\"Found a new / modified file {file_path}\")\n",
    "            changes[\"modified\"].append(file_path)\n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f35309-ca46-4a7e-9643-1943397dfbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_files_in_vector_store(vector_store, files):\n",
    "    if len(files) == 0:\n",
    "        print(\"No new files found\")\n",
    "        return\n",
    "    ids_to_delete = []\n",
    "    for file in files:\n",
    "        for doc in vector_store:\n",
    "            if doc.metadata.get('source') == file:\n",
    "                ids_to_delete.append(doc.id)\n",
    "    vector_store.delete(ids_to_delete)\n",
    "    documents = parse_markdown_files(files)\n",
    "    # Split long documents into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    split_documents = text_splitter.split_documents(documents)\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "    embeddings_list = embeddings.embed_documents(texts)  # Compute embeddings for multiple documents\n",
    "    # Add documents to vector store with computed embeddings\n",
    "    vector_store.add_documents(\n",
    "        documents=split_documents,\n",
    "        embeddings=embeddings_list\n",
    "    )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1a538-97ee-4bbf-b6ec-1ba96152fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the goals for surgery?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb8ee5f-bb8f-4181-8925-63d1337bbe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "if vector_store:\n",
    "    changes = get_changes_in_documents_folder(folder)\n",
    "    vector_store = update_files_in_vector_store(vector_store, changes[\"modified\"])\n",
    "else:\n",
    "    vector_store = create_vector_store_from_markdown_files(folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b93082-8410-4d6f-9d4c-21d115d21938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675abf9f-b362-4d6a-8249-f8430971d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the answer and source documents\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the answer\n",
    "print(\"Answer:\")\n",
    "print(result['result'])\n",
    "\n",
    "# Print the source file references\n",
    "print(\"\\nSource Documents:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(f\"File: {doc.metadata['source']}\")\n",
    "    print(f\"Excerpt: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a61d3b-790f-4383-9509-0b557ce38d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19ca5b-2a99-4f29-b349-a4f45b30a47e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
