{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b412cd",
   "metadata": {},
   "source": [
    "# Tutorial For Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f91f41-afe7-49fa-9859-397009613558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23fd80",
   "metadata": {},
   "source": [
    "### Define the GPT Model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab54142-1414-4d82-a6e1-1a51db624dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "import glob\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "427c7461-6d5c-4125-a2f4-2a57e5b62b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def parse_markdown_files(file_paths) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Parse all the markdown files into Documents.\n",
    "\n",
    "    :param file_paths: list of md file_paths\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        # Create a Document object for each file\n",
    "        documents.append(Document(page_content=content, metadata={\"source\": file_path}))\n",
    "    return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260331ce",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter \n",
    "Utility function in LangChain  for splitting large chunks of text into smaller more manageable pieces while ensuring minimal overlap or fragmentation of meaningful content.\n",
    "\n",
    "### Key Features\n",
    "1. **Recursive Splitting**: \n",
    "   - It splits the text hierarchically using multiple delimiters. The splitting process starts with the most significant delimiter (e.g., paragraph breaks) and progressively moves to less significant ones (e.g., sentence breaks, word breaks).\n",
    "   - This ensures that the text is split cleanly and logically, retaining semantic coherence as much as possible.\n",
    "\n",
    "2. **Customizable Delimiters**:\n",
    "   - You can specify a list of delimiters (e.g., `\\n\\n`, `. `, `, `) for the splitting process.\n",
    "   - The splitter uses these in order, falling back to smaller units if a larger split would result in chunks exceeding the maximum size.\n",
    "\n",
    "3. **Chunk Size and Overlap**:\n",
    "   - `chunk_size`: The maximum length of each text chunk, typically measured in characters.\n",
    "   - `chunk_overlap`: The number of characters to overlap between consecutive chunks. This helps in preserving context when chunks are processed individually.\n",
    "\n",
    "4. **Text Preprocessing**:\n",
    "   - Trims unnecessary whitespace around chunks.\n",
    "   - Ensures no chunk exceeds the defined `chunk_size`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e2de353-2f56-41e4-bf35-4d6c036bd539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: ../docs/all.how_write_tutorials.how_to_guide.md\n",
      "Content: <!-- toc -->\n",
      "\n",
      "- [Tutorials \"Learn X in 60 minutes\"](#tutorials-learn-x-in-60-minutes)\n",
      "  * [What are the goals for each tutorial](#what-are-the-goals-for-each-tutorial)\n",
      "\n",
      "<!-- tocstop -->\n",
      "\n",
      "# Tutorials \"Learn X in 60 minutes\"\n",
      "\n",
      "The goal is to give everything needed for one person to become familiar with a\n",
      "Big data / AI / LLM / data science technology in 60 minutes.\n",
      "\n",
      "- Each tutorial conceptually corresponds to a blog entry.\n",
      "\n",
      "Source: ../docs/all.how_write_tutorials.how_to_guide.md\n",
      "Content: Each tutorial corresponds to a directory in the `//tutorials` repo\n",
      "[https://github.com/causify-ai/tutorials](https://github.com/causify-ai/tutorials)\n",
      "with\n",
      "\n",
      "Source: ../docs/all.how_write_tutorials.how_to_guide.md\n",
      "Content: - A markdown \\`XYZ.API.md\\` about the API and the software layer written by us\n",
      "  on top of the native API\n",
      "- A markdown `XYZ.example.md` with a full example of an application using the\n",
      "  API\n",
      "- A Docker container with everything you need in our Causify dev-system format\n",
      "- A Jupyter notebook with an example of APIs\n",
      "- A Jupyter notebook with a full example\n",
      "\n",
      "## What are the goals for each tutorial\n",
      "\n",
      "Docker container\n",
      "\n",
      "Source: ../docs/all.how_write_tutorials.how_to_guide.md\n",
      "Content: Docker container\n",
      "\n",
      "- Provides a Docker container with everything installed and ready to run\n",
      "  tutorials and develop with that technology\n",
      "  - Often installing the package and get it to work takes long to figure out\n",
      "- All the code is on GitHub in a common format to all tutorials\n",
      "\n",
      "Jupyter notebooks\n",
      "\n",
      "Source: ../docs/all.how_write_tutorials.how_to_guide.md\n",
      "Content: - Each Jupyter notebook should\n",
      "  - Be unit tested so that you are guaranteed that it works\n",
      "    - It's super frustrating when a tutorial doesn't work because the version of\n",
      "      the library is not compatible with the code anymore\n",
      "  - Be self-contained and linear: each example is explained thoroughly without\n",
      "    having to jump from tutorial to tutorial\n",
      "    - Each cell and its output is commented and explained\n",
      "  - Run end-to-end after a restart (we can add a unit test for it)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def list_markdown_files(directory):\n",
    "    return list(glob.glob(f\"{directory}/*.md\"))\n",
    "\n",
    "# Directory containing Markdown files\n",
    "directory = \"../docs\"\n",
    "\n",
    "# List Markdown files\n",
    "markdown_files = list_markdown_files(directory)\n",
    "\n",
    "# Parse Markdown files into LangChain documents\n",
    "documents = parse_markdown_files(markdown_files)\n",
    "\n",
    "# Split long documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Print sample chunked documents\n",
    "for doc in split_documents[:5]:\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e7b90",
   "metadata": {},
   "source": [
    "### VECTOR STORES\n",
    "\n",
    "#### FAISS (Facebook AI Similarity Search) \n",
    "It is a library designed for efficient similarity search and clustering of dense vectors. In LangChain, FAISS is commonly used as a vector store to store and retrieve embeddings, which are vector representations of text or other data.\n",
    "\n",
    "#### Key Features of FAISS Vector Stores:\n",
    "1. Efficient Storage and Search: FAISS stores dense vector embeddings and allows fast retrieval using similarity metrics like cosine similarity or inner product.\n",
    "2. Indexing Options: Supports different types of indexes (e.g., Flat, IVF, HNSW) to balance between accuracy and speed depending on the dataset size and search requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be86c16-f178-42cf-b0c0-911c84c0d7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_372/2866751701.py:5: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Embed and store split_documents\n",
    "vector_store = FAISS.from_documents(split_documents, embeddings)\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a4a0dc-967f-4225-ba19-54f444498bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create the QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8920b1c6-1f29-4092-a4ce-672fdc339033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_372/1908505659.py:5: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The guidelines for creating a new project, as outlined in the context provided, include the following key points:\n",
      "\n",
      "1. Covering various technologies and tools like Git, Docker, Postgres, MongoDB, Airflow, Dask, GitHub, and Spark.\n",
      "2. Clearly defining what the package is, the problem it solves, and discussing alternatives (both open source and commercial) with their advantages and disadvantages.\n",
      "3. Describing the native API of the package.\n",
      "4. Providing a detailed description of the Docker container used in the project.\n",
      "5. Including visual aids such as flow diagrams, data transformation steps, and plots to enhance understanding.\n",
      "6. Referencing books and tutorials that are considered valuable and comprehensive.\n",
      "7. Ensuring that each Jupyter notebook is unit tested, self-contained, linear, and runs end-to-end after a restart.\n",
      "\n",
      "Following these guidelines can help in creating well-structured and informative projects that are easy to understand and work with.\n",
      "\n",
      "Source Documents:\n",
      "File: ../docs/all.how_write_tutorials.how_to_guide.md\n",
      "Excerpt: Markdown documents should cover:...\n",
      "File: ../docs/all.how_write_tutorials.how_to_guide.md\n",
      "Excerpt: This is the same approach we use in DATA605 tutorials\n",
      "https://github.com/gpsaggese/umd\\_data605/tree/main/tutorials, e.g.,\n",
      "\n",
      "- Git\n",
      "- Docker\n",
      "- Docker compose\n",
      "- Postgres\n",
      "- MongoDB\n",
      "- Airflow\n",
      "- Dask\n",
      "- GitH...\n",
      "File: ../docs/all.how_write_tutorials.how_to_guide.md\n",
      "Excerpt: - What it is the package\n",
      "- What problem it solves\n",
      "- What are the alternatives, both open source and commercial with comments about\n",
      "  advantages and disadvantages\n",
      "- Describe the native API\n",
      "- Descriptio...\n",
      "File: ../docs/all.how_write_tutorials.how_to_guide.md\n",
      "Excerpt: - Each Jupyter notebook should\n",
      "  - Be unit tested so that you are guaranteed that it works\n",
      "    - It's super frustrating when a tutorial doesn't work because the version of\n",
      "      the library is not com...\n"
     ]
    }
   ],
   "source": [
    "# User's question\n",
    "query = \"What are the guidelines on creating new project\"\n",
    "\n",
    "# Get the answer and source documents\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the answer\n",
    "print(\"Answer:\")\n",
    "print(result['result'])\n",
    "\n",
    "# Print the source file references\n",
    "print(\"\\nSource Documents:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(f\"File: {doc.metadata['source']}\")\n",
    "    print(f\"Excerpt: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a63c5db-d452-4392-bd4f-c68da5b3b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve vectors by document name\n",
    "def get_vectors_by_document_name(vector_store, document_name):\n",
    "    # Query using the metadata field `source`\n",
    "    results = vector_store.similarity_search(\n",
    "        query=\"\",  # Pass an empty query or a dummy vector if supported\n",
    "        k=None,    # Retrieve all matching documents\n",
    "        filter={\"source\": document_name}  # Filter by the document name\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "document_name = \"all.how_write_tutorials.how_to_guide.md\"\n",
    "results = get_vectors_by_document_name(vector_store, document_name)\n",
    "\n",
    "# Print results\n",
    "for doc in results:\n",
    "    print(f\"File: {doc.metadata['source']}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e68a944",
   "metadata": {},
   "source": [
    "### Demo to create a documentation QA bot but the docs can be updated or deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2057dc61-2130-4395-ad63-9af8690698f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = None\n",
    "folder = \"../docs\"\n",
    "filename_to_md5sum = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8c7458e-e47e-4513-a5e0-caa3b5dae7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers.hsystem as hsystem\n",
    "# Function to parse and structure Markdown files. \n",
    "def parse_markdown_files(file_paths):\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        md5sum, _ = hsystem.system_to_string(f\"md5sum {file_path}\")[1].split()\n",
    "        filename_to_md5sum[file_path] = md5sum\n",
    "        # Create a Document object for each file\n",
    "        documents.append(Document(page_content=content, metadata={\"source\": file_path}))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85ae283e-6aff-49bc-ad21-fe8db669eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "def create_vector_store_from_markdown_files(folder):\n",
    "    # List Markdown files\n",
    "    markdown_files = list_markdown_files(directory)\n",
    "    # Parse Markdown files into LangChain documents\n",
    "    documents = parse_markdown_files(markdown_files)\n",
    "    # Split long documents into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    split_documents = text_splitter.split_documents(documents)\n",
    "    # Create embeddings for all documents.\n",
    "    vector_store = Chroma.from_documents(split_documents, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae0661b8-4e40-4e07-9cbd-b54cad041170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_changes_in_documents_folder(folder):\n",
    "    # List Markdown files\n",
    "    markdown_files = list_markdown_files(folder)\n",
    "    changes = {}\n",
    "    changes[\"modified\"] = []\n",
    "    for file_path in markdown_files:\n",
    "        md5sum, _ = hsystem.system_to_string(f\"md5sum {file_path}\")[1].split()\n",
    "        if file_path not in filename_to_md5sum or filename_to_md5sum[file_path] == md5sum:\n",
    "            print(f\"Found a new / modified file {file_path}\")\n",
    "            changes[\"modified\"].append(file_path)\n",
    "    return changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5f35309-ca46-4a7e-9643-1943397dfbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_files_in_vector_store(vector_store, files):\n",
    "    if len(files) == 0:\n",
    "        print(\"No new files found\")\n",
    "        return\n",
    "    ids_to_delete = []\n",
    "    for file in files:\n",
    "        for doc in vector_store:\n",
    "            if doc.metadata.get('source') == file:\n",
    "                ids_to_delete.append(doc.id)\n",
    "    vector_store.delete(ids_to_delete)\n",
    "    documents = parse_markdown_files(files)\n",
    "    # Split long documents into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    split_documents = text_splitter.split_documents(documents)\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "    embeddings_list = embeddings.embed_documents(texts)  # Compute embeddings for multiple documents\n",
    "    # Add documents to vector store with computed embeddings\n",
    "    vector_store.add_documents(\n",
    "        documents=split_documents,\n",
    "        embeddings=embeddings_list\n",
    "    )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5be1a538-97ee-4bbf-b6ec-1ba96152fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the goals for tutorial project?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcb8ee5f-bb8f-4181-8925-63d1337bbe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "if vector_store:\n",
    "    changes = get_changes_in_documents_folder(folder)\n",
    "    vector_store = update_files_in_vector_store(vector_store, changes[\"modified\"])\n",
    "else:\n",
    "    vector_store = create_vector_store_from_markdown_files(folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10b93082-8410-4d6f-9d4c-21d115d21938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "675abf9f-b362-4d6a-8249-f8430971d2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The goals for the tutorial project are to provide a comprehensive understanding of a Big data / AI / LLM / data science technology within a 60-minute timeframe. Each tutorial includes a markdown file about the API, a markdown file with a full example application, a Docker container in the Causify dev-system format, and Jupyter notebooks with API examples and full examples. The Jupyter notebooks should be unit tested, self-contained, linear, and able to run end-to-end after a restart. The aim is to ensure that the tutorials are functional, easy to follow, and provide a complete learning experience without the need to jump between different tutorials.\n",
      "\n",
      "Source Documents:\n",
      "File: ../docs/all.how_write_tutorials.how_to_guide.md\n",
      "Excerpt: <!-- toc -->\n",
      "\n",
      "- [Tutorials \"Learn X in 60 minutes\"](#tutorials-learn-x-in-60-minutes)\n",
      "  * [What are the goals for each tutorial](#what-are-the-goals-for-each-tutorial)\n",
      "\n",
      "<!-- tocstop -->\n",
      "\n",
      "# Tutorials \"...\n",
      "File: ../docs/all.how_write_tutorials.how_to_guide.md\n",
      "Excerpt: - A markdown \\`XYZ.API.md\\` about the API and the software layer written by us\n",
      "  on top of the native API\n",
      "- A markdown `XYZ.example.md` with a full example of an application using the\n",
      "  API\n",
      "- A Docker...\n",
      "File: ../docs/all.how_write_tutorials.how_to_guide.md\n",
      "Excerpt: - Each Jupyter notebook should\n",
      "  - Be unit tested so that you are guaranteed that it works\n",
      "    - It's super frustrating when a tutorial doesn't work because the version of\n",
      "      the library is not com...\n",
      "File: ../docs/all.how_write_tutorials.how_to_guide.md\n",
      "Excerpt: This is the same approach we use in DATA605 tutorials\n",
      "https://github.com/gpsaggese/umd\\_data605/tree/main/tutorials, e.g.,\n",
      "\n",
      "- Git\n",
      "- Docker\n",
      "- Docker compose\n",
      "- Postgres\n",
      "- MongoDB\n",
      "- Airflow\n",
      "- Dask\n",
      "- GitH...\n"
     ]
    }
   ],
   "source": [
    "# Get the answer and source documents\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the answer\n",
    "print(\"Answer:\")\n",
    "print(result['result'])\n",
    "\n",
    "# Print the source file references\n",
    "print(\"\\nSource Documents:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(f\"File: {doc.metadata['source']}\")\n",
    "    print(f\"Excerpt: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a61d3b-790f-4383-9509-0b557ce38d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19ca5b-2a99-4f29-b349-a4f45b30a47e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
