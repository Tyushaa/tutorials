{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4286b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d87e711c",
   "metadata": {},
   "source": [
    "# Fetch and Store Historical Bitcoin Data in PostgreSQL\n",
    "\n",
    "This notebook demonstrates how to fetch historical Bitcoin (BTC) data from the CryptoCompare API, process the data, and store it in a PostgreSQL database. The process is broken down into the following steps:\n",
    "\n",
    "## Steps:\n",
    "1. **Load Environment Variables**: We load API keys and database URL from an `.env` file.\n",
    "2. **Configure Database Connection**: We set up a connection to the PostgreSQL database using SQLAlchemy.\n",
    "3. **API Interaction**: We fetch historical data for Bitcoin in chunks, using the CryptoCompare API.\n",
    "4. **Data Processing**: The data is processed, including renaming columns, converting timestamps, and handling duplicates.\n",
    "5. **Store in Database**: The processed data is stored in a PostgreSQL database table.\n",
    "\n",
    "### Required Libraries:\n",
    "- `os`: For handling environment variables.\n",
    "- `time`: For managing time intervals.\n",
    "- `pandas`: For data manipulation and storage.\n",
    "- `requests`: For making HTTP requests to the CryptoCompare API.\n",
    "- `dotenv`: For loading environment variables from the `.env` file.\n",
    "- `sqlalchemy`: For connecting to the PostgreSQL database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f48bdf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T04:27:58.080959Z",
     "start_time": "2025-05-12T04:27:38.318252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting sqlalchemy\n",
      "  Downloading sqlalchemy-2.0.40-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.8/dist-packages (2.9.10)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests) (3.10)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting numpy>=1.20.3 (from pandas)\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy) (4.13.2)\n",
      "Collecting greenlet>=1 (from sqlalchemy)\n",
      "  Downloading greenlet-3.1.1-cp38-cp38-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy-2.0.40-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
      "Downloading greenlet-3.1.1-cp38-cp38-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (605 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.0/606.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: pytz, urllib3, tzdata, python-dotenv, numpy, greenlet, charset-normalizer, certifi, sqlalchemy, requests, pandas\n",
      "Successfully installed certifi-2025.4.26 charset-normalizer-3.4.2 greenlet-3.1.1 numpy-1.24.4 pandas-2.0.3 python-dotenv-1.0.1 pytz-2025.2 requests-2.32.3 sqlalchemy-2.0.40 tzdata-2025.2 urllib3-2.2.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv requests pandas sqlalchemy psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1df88de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:08:17.087996Z",
     "start_time": "2025-05-11T10:08:16.148957Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c462a437",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:08:17.099403Z",
     "start_time": "2025-05-11T10:08:17.091078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Env variables\n",
    "load_dotenv()  \n",
    "API_KEY      = os.getenv(\"API_KEY\")\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "\n",
    "if not API_KEY:\n",
    "    raise EnvironmentError(\"API_KEY missing.\")\n",
    "if not DATABASE_URL:\n",
    "    raise EnvironmentError(\"DATABASE_URL missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef568c9a",
   "metadata": {},
   "source": [
    "### PATCH DATABASE URL FOR SQLALCHEMY ≥1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6f9a54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:08:17.346128Z",
     "start_time": "2025-05-11T10:08:17.105350Z"
    }
   },
   "outputs": [],
   "source": [
    "# SQLAlchemy 1.4+ rejects \"postgres://\"; it needs \"postgresql://\"\n",
    "if DATABASE_URL.startswith(\"postgres://\"):\n",
    "    DATABASE_URL = DATABASE_URL.replace(\"postgres://\", \"postgresql://\", 1)\n",
    "\n",
    "# Ensure psycopg2 is installed: pip install psycopg2-binary\n",
    "engine = create_engine(DATABASE_URL, echo=False, future=True)\n",
    "\n",
    "# CONFIG\n",
    "TABLE_NAME   = \"bitcion_daily_data\"\n",
    "HISTORICAL_CHUNK = 2000  # max per-request limit\n",
    "BASE_URL         = \"https://min-api.cryptocompare.com/data/v2/histoday\"\n",
    "HEADERS          = {\"authorization\": f\"Apikey {API_KEY}\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b033fdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T09:38:01.326205Z",
     "start_time": "2025-05-11T09:38:01.320313Z"
    }
   },
   "source": [
    "## Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cad6cd65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:08:17.366734Z",
     "start_time": "2025-05-11T10:08:17.350352Z"
    }
   },
   "outputs": [],
   "source": [
    "# FETCH A CHUNK\n",
    "def fetch_chunk(days: int, to_ts: int = None) -> list:\n",
    "    limit_param = max(days - 1, 1)\n",
    "    params = {\"fsym\": \"BTC\", \"tsym\": \"USD\", \"limit\": limit_param}\n",
    "    if to_ts:\n",
    "        params[\"toTs\"] = to_ts\n",
    "\n",
    "    resp = requests.get(BASE_URL, params=params, headers=HEADERS)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    if data.get(\"Response\") != \"Success\":\n",
    "        raise RuntimeError(f\"API Error: {data.get('Message', 'Unknown')}\")\n",
    "    return data[\"Data\"][\"Data\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d3924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T09:39:15.127230Z",
     "start_time": "2025-05-11T09:39:15.122848Z"
    }
   },
   "source": [
    "## Fetch Full Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac30de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:08:17.389330Z",
     "start_time": "2025-05-11T10:08:17.372900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fetch Full HISTORY\n",
    "def fetch_full_historical(total_days: int) -> pd.DataFrame:\n",
    "    to_ts = int(time.time())\n",
    "    days_left = total_days\n",
    "    all_days = []\n",
    "\n",
    "    while days_left > 0:\n",
    "        batch = min(HISTORICAL_CHUNK, days_left)\n",
    "        chunk = fetch_chunk(batch, to_ts)\n",
    "        if not chunk:\n",
    "            break\n",
    "\n",
    "        all_days.extend(chunk)\n",
    "        to_ts = chunk[0][\"time\"] - 1\n",
    "        days_left -= batch\n",
    "        time.sleep(0.2)  # be kind to the API\n",
    "\n",
    "    df = pd.DataFrame(all_days)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"], unit=\"s\").dt.date\n",
    "    df = df.rename(columns={\n",
    "        \"open\": \"open_usd\",\n",
    "        \"high\": \"high_usd\",\n",
    "        \"low\": \"low_usd\",\n",
    "        \"close\": \"close_usd\",\n",
    "        \"volumeto\": \"volume_usd\"\n",
    "    })\n",
    "    return (\n",
    "        df[[\"date\", \"open_usd\", \"high_usd\", \"low_usd\", \"close_usd\", \"volume_usd\"]]\n",
    "        .drop_duplicates(\"date\")\n",
    "        .sort_values(\"date\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3cc5e3",
   "metadata": {},
   "source": [
    "## Storing Data in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02be0854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T10:09:08.049727Z",
     "start_time": "2025-05-11T10:08:53.980541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching ~4383 days of BTC/USD data (~12.00 years)...\n",
      "Fetched 4383 records. Writing to database…\n",
      "✅ Data saved to table 'bitcion_daily_data' in your database.\n",
      "         date  open_usd  high_usd  low_usd  close_usd   volume_usd\n",
      "0  2013-05-12    115.64    117.47   112.40     114.82   2357929.41\n",
      "1  2013-05-13    114.82    118.88   114.50     117.98   3058207.49\n",
      "2  2013-05-14    117.98    119.80   109.42     111.40  10075279.73\n",
      "3  2013-05-15    111.40    116.44   103.02     114.22  12997994.80\n",
      "4  2013-05-16    114.22    118.97   112.10     118.21   5202992.37\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "def main():\n",
    "    try:\n",
    "        # Number of year of data you want to fetch\n",
    "        num_years = 12\n",
    "        total_days = int(num_years * 365.25)\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a numeric value.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFetching ~{total_days} days of BTC/USD data (~{num_years:.2f} years)...\")\n",
    "    df = fetch_full_historical(total_days)\n",
    "    print(f\"Fetched {len(df)} records. Writing to database…\")\n",
    "\n",
    "    # write to Postgres (will replace any existing table with the same name)\n",
    "    df.to_sql(TABLE_NAME, engine, if_exists=\"replace\", index=False)\n",
    "    print(f\"✅ Data saved to table '{TABLE_NAME}' in your database.\")\n",
    "\n",
    "    # Optional: preview first few rows\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e07a3",
   "metadata": {},
   "source": [
    "# Live Data Fetch\n",
    "\n",
    "This notebook demonstrates how to wrap the CryptoCompare “histominute” endpoint to:\n",
    "\n",
    "1. **Fetch** full historical minute-level Bitcoin data (past 2 days).\n",
    "2. **Fetch** real-time Bitcoin data in one-day windows.\n",
    "3. **Append** new observations into an existing CSV.\n",
    "\n",
    "We’ll build helper functions, show their usage, and finally run a one-off historical download.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2289344",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T04:27:58.657502Z",
     "start_time": "2025-05-12T04:27:58.090504Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, inspect, text\n",
    "\n",
    "# LOAD ENV VARS\n",
    "load_dotenv()  # pip install python-dotenv\n",
    "API_KEY      = os.getenv(\"API_KEY\")\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise EnvironmentError(\"API_KEY missing\")\n",
    "if not DATABASE_URL:\n",
    "    raise EnvironmentError(\"DATABASE_URL missing\")\n",
    "\n",
    "\n",
    "# Fix for SQLAlchemy ≥1.4\n",
    "if DATABASE_URL.startswith(\"postgres://\"):\n",
    "    DATABASE_URL = DATABASE_URL.replace(\"postgres://\", \"postgresql://\", 1)\n",
    "\n",
    "engine = create_engine(DATABASE_URL, echo=False, future=True)\n",
    "\n",
    "# CONFIG \n",
    "TABLE_NAME        = \"btc_minute_data123\"\n",
    "HISTORICAL_DAYS   = 7\n",
    "HISTORICAL_MIN    = HISTORICAL_DAYS * 24 * 60  # total minutes to fetch initially\n",
    "HISTORICAL_CHUNK  = 2000                      # max per-request limit\n",
    "REALTIME_LIMIT    = 24 * 60                   # last 24h, single request\n",
    "MAX_ROWS          = 2000\n",
    "\n",
    "BASE_URL = \"https://min-api.cryptocompare.com/data/v2/histominute\"\n",
    "HEADERS  = {\"authorization\": f\"Apikey {API_KEY}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369ccf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T17:47:37.749489Z",
     "start_time": "2025-05-11T17:47:37.742912Z"
    }
   },
   "source": [
    "##  Code for fetch_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3412fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T04:27:58.665613Z",
     "start_time": "2025-05-12T04:27:58.660121Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_chunk(limit: int, to_ts: int = None) -> list:\n",
    "    params = {\"fsym\": \"BTC\", \"tsym\": \"USD\", \"limit\": limit}\n",
    "    if to_ts:\n",
    "        params[\"toTs\"] = to_ts\n",
    "\n",
    "    r = requests.get(BASE_URL, params=params, headers=HEADERS)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    if data.get(\"Response\") != \"Success\":\n",
    "        raise RuntimeError(f\"API Error: {data.get('Message', 'Unknown')}\")\n",
    "    return data[\"Data\"][\"Data\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e4e44a",
   "metadata": {},
   "source": [
    "## Historical Data Fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e2dc035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T04:27:58.677987Z",
     "start_time": "2025-05-12T04:27:58.669746Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_full_historical(total_minutes: int) -> pd.DataFrame:\n",
    "    to_ts = int(time.time())\n",
    "    left = total_minutes\n",
    "    bars = []\n",
    "\n",
    "    while left > 0:\n",
    "        batch_size = min(HISTORICAL_CHUNK, left)\n",
    "        chunk = fetch_chunk(batch_size, to_ts)\n",
    "        if not chunk:\n",
    "            break\n",
    "        bars.extend(chunk)\n",
    "        to_ts = chunk[0][\"time\"] - 1\n",
    "        left -= batch_size\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(bars)\n",
    "    df[\"timestamp\"]  = pd.to_datetime(df[\"time\"], unit=\"s\")\n",
    "    df[\"price_usd\"]  = df[\"close\"]\n",
    "    df[\"volume_usd\"] = df[\"volumeto\"]\n",
    "    df[\"volume_btc\"] = df[\"volumefrom\"]\n",
    "\n",
    "    return (\n",
    "        df[[\"timestamp\", \"price_usd\", \"volume_usd\", \"volume_btc\"]]\n",
    "        .drop_duplicates(subset=\"timestamp\")\n",
    "        .sort_values(\"timestamp\")\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167d0625",
   "metadata": {},
   "source": [
    "## Real-Time Data Fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1481492d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T04:27:58.687533Z",
     "start_time": "2025-05-12T04:27:58.682920Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_realtime_df() -> pd.DataFrame:\n",
    "    bars = fetch_chunk(REALTIME_LIMIT)\n",
    "    df = pd.DataFrame(bars)\n",
    "    df[\"timestamp\"]  = pd.to_datetime(df[\"time\"], unit=\"s\")\n",
    "    df[\"price_usd\"]  = df[\"close\"]\n",
    "    df[\"volume_usd\"] = df[\"volumeto\"]\n",
    "    df[\"volume_btc\"] = df[\"volumefrom\"]\n",
    "    return df[[\"timestamp\", \"price_usd\", \"volume_usd\", \"volume_btc\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756fade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T17:57:02.669596Z",
     "start_time": "2025-05-11T17:57:02.665735Z"
    }
   },
   "source": [
    "## DATABASE UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd8515e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T04:27:58.696969Z",
     "start_time": "2025-05-12T04:27:58.692377Z"
    }
   },
   "outputs": [],
   "source": [
    "def ensure_table_exists():\n",
    "    insp = inspect(engine)\n",
    "    if not insp.has_table(TABLE_NAME):\n",
    "        print(f\"Table '{TABLE_NAME}' not found—inserting initial {HISTORICAL_DAYS}-day history…\")\n",
    "        df0 = fetch_full_historical(HISTORICAL_MIN)\n",
    "        df0.to_sql(TABLE_NAME, engine, if_exists=\"replace\", index=False)\n",
    "        print(f\"  → Inserted {len(df0)} rows into '{TABLE_NAME}'.\")\n",
    "    else:\n",
    "        print(f\"Table '{TABLE_NAME}' already exists, skipping initial load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b529d2a0",
   "metadata": {},
   "source": [
    "## REMOVE OLD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fabeaeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:03:55.838805Z",
     "start_time": "2025-05-12T04:27:58.698568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching initial 7-day dataset…\n",
      "Saved 10080 rows to btc_data.csv\n",
      "[2025-05-12 04:28:09] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10081\n",
      "[2025-05-12 04:29:11] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10082\n",
      "[2025-05-12 04:30:13] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10083\n",
      "[2025-05-12 04:31:14] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10084\n",
      "[2025-05-12 04:32:16] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10085\n",
      "[2025-05-12 04:33:19] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10086\n",
      "[2025-05-12 04:34:21] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10087\n",
      "[2025-05-12 04:35:22] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10088\n",
      "[2025-05-12 04:36:24] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10089\n",
      "[2025-05-12 04:37:25] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10090\n",
      "[2025-05-12 04:38:27] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10091\n",
      "[2025-05-12 04:39:28] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10092\n",
      "[2025-05-12 04:40:30] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10093\n",
      "[2025-05-12 04:41:32] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10094\n",
      "[2025-05-12 04:42:33] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10095\n",
      "[2025-05-12 04:43:35] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10096\n",
      "[2025-05-12 04:44:37] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10097\n",
      "[2025-05-12 04:45:39] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10098\n",
      "[2025-05-12 04:46:40] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10099\n",
      "[2025-05-12 04:47:42] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10100\n",
      "[2025-05-12 04:48:44] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10101\n",
      "[2025-05-12 04:50:16] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10103\n",
      "[2025-05-12 04:51:18] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10104\n",
      "[2025-05-12 04:52:19] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10105\n",
      "[2025-05-12 04:53:21] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10106\n",
      "[2025-05-12 04:54:22] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10107\n",
      "[2025-05-12 04:55:24] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10108\n",
      "[2025-05-12 05:22:35] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10135\n",
      "[2025-05-12 05:23:40] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10136\n",
      "[2025-05-12 05:24:41] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10137\n",
      "[2025-05-12 05:25:44] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10138\n",
      "[2025-05-12 05:26:46] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10139\n",
      "[2025-05-12 05:27:48] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10140\n",
      "[2025-05-12 05:28:49] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10141\n",
      "[2025-05-12 05:29:51] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10142\n",
      "[2025-05-12 05:30:54] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10143\n",
      "[2025-05-12 05:31:55] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10144\n",
      "[2025-05-12 05:32:57] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10145\n",
      "[2025-05-12 05:33:59] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10147\n",
      "[2025-05-12 05:35:03] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10148\n",
      "[2025-05-12 05:36:04] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10149\n",
      "[2025-05-12 05:37:06] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10150\n",
      "[2025-05-12 05:38:08] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10151\n",
      "[2025-05-12 05:39:10] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10152\n",
      "[2025-05-12 05:40:13] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10153\n",
      "[2025-05-12 05:41:14] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10154\n",
      "[2025-05-12 05:42:16] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10155\n",
      "[2025-05-12 05:43:18] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10156\n",
      "[2025-05-12 05:44:21] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10157\n",
      "[2025-05-12 05:45:22] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10158\n",
      "[2025-05-12 05:46:23] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10159\n",
      "[2025-05-12 05:47:25] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10160\n",
      "[2025-05-12 05:48:29] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10161\n",
      "[2025-05-12 05:49:30] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10162\n",
      "[2025-05-12 05:50:34] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10163\n",
      "[2025-05-12 05:51:36] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10164\n",
      "[2025-05-12 05:52:39] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10165\n",
      "[2025-05-12 05:53:40] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10166\n",
      "[2025-05-12 05:54:42] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10167\n",
      "[2025-05-12 05:55:44] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10168\n",
      "[2025-05-12 05:56:46] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10169\n",
      "[2025-05-12 05:57:48] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10170\n",
      "[2025-05-12 05:58:51] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10171\n",
      "[2025-05-12 05:59:54] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10172\n",
      "[2025-05-12 06:00:57] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10173\n",
      "[2025-05-12 06:01:58] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10174\n",
      "[2025-05-12 06:03:00] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10176\n",
      "[2025-05-12 06:04:02] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10177\n",
      "[2025-05-12 06:05:04] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10178\n",
      "[2025-05-12 06:06:06] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10179\n",
      "[2025-05-12 06:07:08] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10180\n",
      "[2025-05-12 06:08:09] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10181\n",
      "[2025-05-12 06:09:11] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10182\n",
      "[2025-05-12 06:10:14] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10183\n",
      "[2025-05-12 06:11:15] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10184\n",
      "[2025-05-12 06:12:18] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10185\n",
      "[2025-05-12 06:13:22] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10186\n",
      "[2025-05-12 06:14:24] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10187\n",
      "[2025-05-12 06:15:26] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10188\n",
      "[2025-05-12 06:16:27] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10189\n",
      "[2025-05-12 06:17:29] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10190\n",
      "[2025-05-12 06:18:30] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10191\n",
      "[2025-05-12 06:19:32] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10192\n",
      "[2025-05-12 06:20:34] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10193\n",
      "[2025-05-12 06:21:35] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10194\n",
      "[2025-05-12 06:22:38] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10195\n",
      "[2025-05-12 06:23:40] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10196\n",
      "[2025-05-12 06:24:42] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10197\n",
      "[2025-05-12 06:25:43] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10198\n",
      "[2025-05-12 06:26:45] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10199\n",
      "[2025-05-12 06:27:46] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10200\n",
      "[2025-05-12 06:28:49] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10201\n",
      "[2025-05-12 06:29:50] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10202\n",
      "[2025-05-12 06:30:52] Fetching last 1440 minutes…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended new data. Total rows: 10203\n",
      "[2025-05-12 06:31:54] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10204\n",
      "[2025-05-12 06:32:55] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10205\n",
      "[2025-05-12 06:33:58] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10206\n",
      "[2025-05-12 06:35:00] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10208\n",
      "[2025-05-12 06:36:02] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10209\n",
      "[2025-05-12 06:37:03] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10210\n",
      "[2025-05-12 06:38:06] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10211\n",
      "[2025-05-12 06:39:08] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10212\n",
      "[2025-05-12 06:40:10] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10213\n",
      "[2025-05-12 06:41:12] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10214\n",
      "[2025-05-12 06:42:13] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10215\n",
      "[2025-05-12 06:43:15] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10216\n",
      "[2025-05-12 06:44:17] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10217\n",
      "[2025-05-12 06:45:18] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10218\n",
      "[2025-05-12 06:46:19] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10219\n",
      "[2025-05-12 06:47:21] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10220\n",
      "[2025-05-12 06:48:22] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10221\n",
      "[2025-05-12 06:49:24] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10222\n",
      "[2025-05-12 06:50:25] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10223\n",
      "[2025-05-12 06:51:27] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10224\n",
      "[2025-05-12 06:52:29] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10225\n",
      "[2025-05-12 06:53:30] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10226\n",
      "[2025-05-12 06:54:32] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10227\n",
      "[2025-05-12 06:55:35] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10228\n",
      "[2025-05-12 06:56:36] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10229\n",
      "[2025-05-12 06:57:39] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10230\n",
      "[2025-05-12 06:58:40] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10231\n",
      "[2025-05-12 06:59:42] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10232\n",
      "[2025-05-12 07:00:43] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10233\n",
      "[2025-05-12 07:01:44] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10234\n",
      "[2025-05-12 07:02:46] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10235\n",
      "[2025-05-12 07:03:48] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10236\n",
      "[2025-05-12 07:04:49] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10237\n",
      "[2025-05-12 07:05:51] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10238\n",
      "[2025-05-12 07:06:52] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10239\n",
      "[2025-05-12 07:07:53] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10240\n",
      "[2025-05-12 07:08:55] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10241\n",
      "[2025-05-12 07:09:56] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10242\n",
      "[2025-05-12 07:10:58] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10243\n",
      "[2025-05-12 07:11:59] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10245\n",
      "[2025-05-12 07:13:00] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10246\n",
      "[2025-05-12 07:14:03] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10247\n",
      "[2025-05-12 07:15:05] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10248\n",
      "[2025-05-12 07:16:08] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10249\n",
      "[2025-05-12 07:17:11] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10250\n",
      "[2025-05-12 07:18:13] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10251\n",
      "[2025-05-12 07:19:14] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10252\n",
      "[2025-05-12 07:20:16] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10253\n",
      "[2025-05-12 07:21:17] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10254\n",
      "[2025-05-12 07:22:19] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10255\n",
      "[2025-05-12 07:23:20] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10256\n",
      "[2025-05-12 07:24:22] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10257\n",
      "[2025-05-12 07:25:24] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10258\n",
      "[2025-05-12 07:26:26] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10259\n",
      "[2025-05-12 07:27:29] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10260\n",
      "[2025-05-12 07:28:30] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10261\n",
      "[2025-05-12 07:29:32] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10262\n",
      "[2025-05-12 07:30:33] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10263\n",
      "[2025-05-12 07:31:35] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10264\n",
      "[2025-05-12 07:32:36] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10265\n",
      "[2025-05-12 07:33:38] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10266\n",
      "[2025-05-12 07:34:40] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10267\n",
      "[2025-05-12 07:35:41] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10268\n",
      "[2025-05-12 07:36:45] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10269\n",
      "[2025-05-12 07:37:46] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10270\n",
      "[2025-05-12 07:38:48] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10271\n",
      "[2025-05-12 07:39:50] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10272\n",
      "[2025-05-12 07:40:51] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10273\n",
      "[2025-05-12 07:41:52] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10274\n",
      "[2025-05-12 07:42:56] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10275\n",
      "[2025-05-12 07:43:57] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10276\n",
      "[2025-05-12 07:44:59] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10277\n",
      "[2025-05-12 07:46:00] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10279\n",
      "[2025-05-12 07:47:03] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10280\n",
      "[2025-05-12 07:48:05] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10281\n",
      "[2025-05-12 07:49:06] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10282\n",
      "[2025-05-12 07:50:08] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10283\n",
      "[2025-05-12 07:51:09] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10284\n",
      "[2025-05-12 07:52:11] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10285\n",
      "[2025-05-12 07:53:12] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10286\n",
      "[2025-05-12 07:54:14] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10287\n",
      "[2025-05-12 07:55:15] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10288\n",
      "[2025-05-12 07:56:17] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10289\n",
      "[2025-05-12 07:57:22] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10290\n",
      "[2025-05-12 07:58:23] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10291\n",
      "[2025-05-12 07:59:25] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10292\n",
      "[2025-05-12 08:00:27] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10293\n",
      "[2025-05-12 08:01:31] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10294\n",
      "[2025-05-12 08:02:33] Fetching last 1440 minutes…\n",
      "Appended new data. Total rows: 10295\n",
      "[2025-05-12 08:03:34] Fetching last 1440 minutes…\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='min-api.cryptocompare.com', port=443): Max retries exceeded with url: /data/v2/histominute?fsym=BTC&tsym=USD&limit=1440 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x77e320751280>: Failed to resolve 'min-api.cryptocompare.com' ([Errno -3] Temporary failure in name resolution)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:918\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    917\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 918\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    919\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connection.py:206\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x77e320751280>: Failed to resolve 'min-api.cryptocompare.com' ([Errno -3] Temporary failure in name resolution)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='min-api.cryptocompare.com', port=443): Max retries exceeded with url: /data/v2/histominute?fsym=BTC&tsym=USD&limit=1440 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x77e320751280>: Failed to resolve 'min-api.cryptocompare.com' ([Errno -3] Temporary failure in name resolution)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Fetching last \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREALTIME_LIMIT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mappend_new_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(CSV_FILE, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAppended new data. Total rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m, in \u001b[0;36mappend_new_data\u001b[0;34m(existing_df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mappend_new_data\u001b[39m(existing_df:pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m----> 2\u001b[0m     new_df \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_realtime_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([existing_df, new_df])\u001b[38;5;241m.\u001b[39mdrop_duplicates(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36mfetch_realtime_df\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfetch_realtime_df\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m----> 2\u001b[0m     bars \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREALTIME_LIMIT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(bars)\n\u001b[1;32m      4\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m], unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mfetch_chunk\u001b[0;34m(limit, to_ts)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m to_ts: \n\u001b[1;32m      4\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoTs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m to_ts\n\u001b[0;32m----> 6\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHEADERS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m      8\u001b[0m payload \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='min-api.cryptocompare.com', port=443): Max retries exceeded with url: /data/v2/histominute?fsym=BTC&tsym=USD&limit=1440 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x77e320751280>: Failed to resolve 'min-api.cryptocompare.com' ([Errno -3] Temporary failure in name resolution)\"))"
     ]
    }
   ],
   "source": [
    "def prune_old_rows(conn):\n",
    "    total = conn.execute(text(f\"SELECT COUNT(*) FROM {TABLE_NAME}\")).scalar_one()\n",
    "    if total > MAX_ROWS:\n",
    "        to_delete = total - MAX_ROWS\n",
    "        conn.execute(text(f\"\"\"\n",
    "            DELETE FROM {TABLE_NAME}\n",
    "            WHERE ctid IN (\n",
    "                SELECT ctid\n",
    "                FROM {TABLE_NAME}\n",
    "                ORDER BY timestamp ASC\n",
    "                LIMIT :n\n",
    "            )\n",
    "        \"\"\"), {\"n\": to_delete})\n",
    "        conn.commit()\n",
    "        print(f\"Pruned {to_delete} old rows; now {MAX_ROWS} rows remain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93557d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:03:55.845690Z",
     "start_time": "2025-05-12T08:03:55.845652Z"
    }
   },
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9662d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ensure_table_exists()\n",
    "\n",
    "    print(\"⏱️  Entering realtime update loop (every 60s)…\")\n",
    "    while True:\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"[{now}] Fetching last 24h of bars…\", end=\" \")\n",
    "\n",
    "        new_df = fetch_realtime_df()\n",
    "\n",
    "        with engine.begin() as conn:\n",
    "            # get latest timestamp in DB\n",
    "            result = conn.execute(text(f\"SELECT MAX(timestamp) FROM {TABLE_NAME}\"))\n",
    "            max_ts = result.scalar_one()\n",
    "            if max_ts is not None:\n",
    "                new_df = new_df[new_df[\"timestamp\"] > max_ts]\n",
    "\n",
    "            if not new_df.empty:\n",
    "                new_df.to_sql(TABLE_NAME, conn, if_exists=\"append\", index=False)\n",
    "                # dedupe any overlapping timestamps\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    DELETE FROM {TABLE_NAME} a\n",
    "                    USING {TABLE_NAME} b\n",
    "                    WHERE a.timestamp = b.timestamp\n",
    "                      AND a.ctid < b.ctid\n",
    "                \"\"\"))\n",
    "                prune_old_rows(conn)\n",
    "                print(f\"Appended {len(new_df)} new rows.\")\n",
    "            else:\n",
    "                print(\"No new bars to append.\")\n",
    "\n",
    "        time.sleep(60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
